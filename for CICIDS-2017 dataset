# ============================================================
# CICIDS-2017 dataset
# KaggleHub -> load CSV(s) -> preprocess -> xgboost.train  -> Calibrated SVM -> Ensemble
# ============================================================

import os, glob, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import kagglehub

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve, auc
)
import matplotlib.pyplot as plt

import xgboost as xgb



MAX_ROWS = 200000        # reduce to 100000 if still crashes
NTHREAD_XGB = 2          # reduce to 1 if still crashes
SVM_SAMPLES = 15000      # reduce to 8000 if slow

print("xgboost version:", xgb.__version__)


path = kagglehub.dataset_download("chethuhn/network-intrusion-dataset")
print("Path to dataset files:", path)


def find_csvs(root_path: str):
    if os.path.isfile(root_path) and root_path.lower().endswith(".csv"):
        return [root_path]
    return sorted(glob.glob(os.path.join(root_path, "**", "*.csv"), recursive=True))

def load_network_csvs(root_path: str, max_rows=None) -> pd.DataFrame:
    files = find_csvs(root_path)
    if not files:
        raise FileNotFoundError(f"No CSV files found under: {root_path}")

    print(f"Found {len(files)} CSV file(s). Loading...")
    dfs = []
    for f in files:
        print("  ->", os.path.basename(f))
        d = pd.read_csv(f, low_memory=False)
        dfs.append(d)

    df = pd.concat(dfs, axis=0, ignore_index=True)

    df.columns = [c.strip() for c in df.columns]
    df.replace([np.inf, -np.inf, "Infinity", "inf", "Inf"], np.nan, inplace=True)
    df.dropna(axis=0, inplace=True)
    df.drop_duplicates(inplace=True)

    if max_rows is not None and len(df) > max_rows:
        df = df.sample(n=max_rows, random_state=42).reset_index(drop=True)
        print(f"Sampled down to {max_rows} rows for RAM safety.")

    return df

df = load_network_csvs(path, max_rows=MAX_ROWS)
print("Loaded shape:", df.shape)


def detect_label_column(df: pd.DataFrame):
    candidates = ["Label", "label", "Class", "class", "target", "Target", "y", "Y"]
    for c in candidates:
        if c in df.columns:
            return c
    return df.columns[-1]

def make_binary_y(df: pd.DataFrame, label_col: str):
    s = df[label_col]

    if pd.api.types.is_numeric_dtype(s):
        y = s.astype(int).values
        uniq = np.unique(y)
        if len(uniq) > 2:
            benign_val = 0 if 0 in uniq else uniq.min()
            y = np.where(y == benign_val, 0, 1).astype(int)
        return y

    s2 = s.astype(str).str.strip().str.upper()
    benign_names = {"BENIGN", "NORMAL", "0"}
    y = np.where(s2.isin(benign_names), 0, 1).astype(int)
    return y

label_col = detect_label_column(df)
print("Detected label column:", label_col)

y = make_binary_y(df, label_col)
uniq, cnt = np.unique(y, return_counts=True)
print("Label distribution:", dict(zip(uniq.tolist(), cnt.tolist())))
if len(uniq) < 2:
    raise ValueError(
        "Only one class after mapping. Set label_col manually or adjust benign_names mapping."
    )


def build_leak_safe_X(df: pd.DataFrame, label_col: str):
    X = df.drop(columns=[label_col], errors="ignore")

    drop_exact = [
        "Flow ID", "Src IP", "Dst IP", "Source IP", "Destination IP",
        "Src Port", "Dst Port", "Source Port", "Destination Port",
        "Timestamp"
    ]
    X = X.drop(columns=[c for c in drop_exact if c in X.columns], errors="ignore")

    id_keywords = [
        "timestamp","time","date","flow id","flowid","session","conn",
        "srcip","dstip","src ip","dst ip","ip",
        "srcport","dstport","src port","dst port","port",
        "mac","imei","imsi","uid","uuid"
    ]
    cols_lower = {c: c.lower() for c in X.columns}
    id_cols = [c for c in X.columns if any(k in cols_lower[c] for k in id_keywords)]
    X = X.drop(columns=list(set(id_cols)), errors="ignore")

    for c in X.columns:
        if X[c].dtype == "object":
            X[c] = pd.to_numeric(X[c], errors="ignore")

    for c in X.select_dtypes(include=["float64"]).columns:
        X[c] = X[c].astype("float32")

    return X

X = build_leak_safe_X(df, label_col)
print("Final X shape:", X.shape)


def make_preprocessor(X_df: pd.DataFrame):
    cat_cols = X_df.select_dtypes(include=["object", "category", "bool"]).columns.tolist()
    num_cols = [c for c in X_df.columns if c not in cat_cols]
    cat_transformer = OneHotEncoder(handle_unknown="ignore", max_categories=50)
    return ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), num_cols),
            ("cat", cat_transformer, cat_cols),
        ],
        remainder="drop"
    )

def scale_pos_weight(y_arr):
    neg = np.sum(y_arr == 0)
    pos = np.sum(y_arr == 1)
    return float(neg / pos) if pos > 0 else 1.0


X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)
X_tr_sub, X_val, y_tr_sub, y_val = train_test_split(X_tr, y_tr, test_size=0.20, stratify=y_tr, random_state=42)


prep = make_preprocessor(X_tr_sub)

X_tr_sub_t = prep.fit_transform(X_tr_sub)
X_val_t    = prep.transform(X_val)

spw = scale_pos_weight(y_tr_sub)

dtrain = xgb.DMatrix(X_tr_sub_t, label=y_tr_sub)
dval   = xgb.DMatrix(X_val_t, label=y_val)

params = {
    "objective": "binary:logistic",
    "eval_metric": "auc",
    "tree_method": "hist",
    "eta": 0.03,
    "max_depth": 8,
    "subsample": 0.85,
    "colsample_bytree": 0.85,
    "min_child_weight": 2,
    "gamma": 0.0,
    "lambda": 3.0,   # reg_lambda
    "alpha": 0.0,    # reg_alpha
    "scale_pos_weight": spw,
    "verbosity": 1,
    "nthread": NTHREAD_XGB,
    "seed": 42,
}

print("\nTraining XGBoost with xgb.train() + early stopping (version-safe)...")
booster = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=5000,
    evals=[(dtrain, "train"), (dval, "valid")],
    early_stopping_rounds=100,
    verbose_eval=200
)


px_val = booster.predict(dval, iteration_range=(0, booster.best_iteration + 1))
print("VAL AUC (XGB):", roc_auc_score(y_val, px_val))


svm_n = min(SVM_SAMPLES, len(X_tr_sub))
X_svm, _, y_svm, _ = train_test_split(
    X_tr_sub, y_tr_sub,
    train_size=svm_n,
    stratify=y_tr_sub,
    random_state=42
)

svm_base = Pipeline([
    ("prep", make_preprocessor(X_svm)),
    ("clf", SVC(kernel="rbf", C=8, gamma="scale", class_weight="balanced"))
])
svm_cal = CalibratedClassifierCV(svm_base, method="sigmoid", cv=3)

print("\nTraining calibrated SVM (subsampled)...")
svm_cal.fit(X_svm, y_svm)

ps_val = svm_cal.predict_proba(X_val)[:, 1]
print("VAL AUC (SVM-cal):", roc_auc_score(y_val, ps_val))


def best_threshold(y_true, prob, metric="accuracy"):
    ths = np.linspace(0.01, 0.99, 199)
    best = (-1, 0.5)
    for t in ths:
        pred = (prob >= t).astype(int)
        if metric == "accuracy":
            s = accuracy_score(y_true, pred)
        elif metric == "f1":
            s = f1_score(y_true, pred, zero_division=0)
        else:
            s = accuracy_score(y_true, pred)
        if s > best[0]:
            best = (s, t)
    return best

best_cfg, best_acc = None, -1.0
for wx in np.linspace(0.70, 0.98, 29):
    p_ens = wx * px_val + (1 - wx) * ps_val
    acc, thr = best_threshold(y_val, p_ens, metric="accuracy")
    if acc > best_acc:
        best_acc = acc
        best_cfg = (float(wx), float(thr))

wx, thr = best_cfg
print("\nBest VAL ensemble (wx, thr):", best_cfg, "VAL Accuracy:", best_acc)


prep_full = make_preprocessor(X_tr)
X_tr_t = prep_full.fit_transform(X_tr)
X_te_t = prep_full.transform(X_te)

X_tr2, X_val2, y_tr2, y_val2 = train_test_split(
    X_tr_t, y_tr, test_size=0.20, stratify=y_tr, random_state=42
)

spw_full = scale_pos_weight(y_tr2)

dtrain2 = xgb.DMatrix(X_tr2, label=y_tr2)
dval2   = xgb.DMatrix(X_val2, label=y_val2)
dtest   = xgb.DMatrix(X_te_t, label=y_te)

params2 = dict(params)
params2["scale_pos_weight"] = spw_full

print("\nFinal training XGBoost (full train) with early stopping...")
booster_final = xgb.train(
    params=params2,
    dtrain=dtrain2,
    num_boost_round=5000,
    evals=[(dtrain2, "train"), (dval2, "valid")],
    early_stopping_rounds=100,
    verbose_eval=200
)


px_te = booster_final.predict(dtest, iteration_range=(0, booster_final.best_iteration + 1))
ps_te = svm_cal.predict_proba(X_te)[:, 1]

p_ens_te = wx * px_te + (1 - wx) * ps_te
pred_ens = (p_ens_te >= thr).astype(int)

def print_metrics(name, y_true, pred, prob):
    print(f"\n{name}")
    print(f"Accuracy  : {accuracy_score(y_true, pred)*100:.2f}%")
    print(f"Precision : {precision_score(y_true, pred, zero_division=0)*100:.2f}%")
    print(f"Recall    : {recall_score(y_true, pred, zero_division=0)*100:.2f}%")
    print(f"F1-score  : {f1_score(y_true, pred, zero_division=0)*100:.2f}%")
    print(f"AUC-ROC   : {roc_auc_score(y_true, prob)*100:.2f}%")

print_metrics("Ensemble (Test)", y_te, pred_ens, p_ens_te)
print("\nConfusion Matrix:\n", confusion_matrix(y_te, pred_ens))


fpr, tpr, _ = roc_curve(y_te, p_ens_te)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.4f}")
plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve (Ensemble)")
plt.legend(loc="lower right")
plt.show()
